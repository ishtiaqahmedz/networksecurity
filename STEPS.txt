created nenv,gitignore,readme,requirements.txt, then .github/workflows folder under whihc "main.yaml" file (to use github actions
in future), created network_data folder, and notebooks folder, networksecurity underwhich we created our standard folder structure
along with dockerfile and setup.py etc files. After that, we created ".env" file, for using environment variables for whcih we will
use python-dotenv package. 
Now updated requirements.txt file, and use <pip install -r requirements.txt>. Clicked on AutoSave option under FILE menu, so the 
project get saved in case of any changes. We setup  the packages by creating "__init__.py" file under the created folders. We setup
the git repo locally and on github adn pushed the changes.

* updated setup.py, introduced "-e ." in requirements.txt so setup.py may out python project as package. after completing setup.py,
we ran pip install -r requirements.txt, it installed the packages and in last, due to presence of "-e .", it refered to setup.py,
made instllations of our developed packages as well. 
We can add this "-e ." in last when project is matured, this is comunicated by the teacher however ChatGPT and Co-pilot suggest
to add it at begining of the project. 
Now after setup, we work on logging. our folder logging will act as a package as we have __init__.py in it. we created loggin.py 
in that folder. then created exception.py - we completed the code within that and checked if it is working by running 
<python exception.py>

********** ETL PIPELINE - Extraction, Transformation and Load *************

Data may be extracted from different apis, s3 buckets, websites - may be processed adn cleaned and then it can be saved in data desitnations
like MongoDB, MySQL etc. This is usually done by Data Engineering Team 

Our problem Statement: check the website if it is pishing or a legit website. We will read the local available data, do some 
processing and load into MongoDB ATLAS.

I created account, created database on MongoDB, for connecting with MongoDB driver, i selected  python 3.6 or later,
it provided pip command for installation of driver, updated pymongo in requirements.txt, click done. Once cluster is created,
go to CONNECT, click driver, click view full code sample, copy the code and paste into push_data.py in your project, changed
password in the code, saved. python -m pip install "pymongo[srv]==3.6" (as given by MongoDB), changed the working directory 
in the terminal and ran the test_mongodb_conn.py to check if the  connection is pinging. But these details should not be hardcoded in our file
so we will create .env file, so we will create MongoDB URL Key and save our password in it. 

we created push_data.py, loaded .env file, we imported certifi package which is used to issue ssl/tls certificate for network security purpose.
completed and ran push_data.py. then created data_ingestion.py, then config_entity.py and start working on it
under constant, created training_pipeline folder and created __init__.py and start working in this init file - created data_ingestion
block there. 

(Note: Instructor added 0.0.0.0/0 - a generic ip address in MongoDB's IP addressses, he said he did it to make http  connection 
and to cater requests.)
Then we moved back to config_entity.py and worked on it, after that we started working on data_ingestion.py, 

created dataclass DataIngestionArtifact in entity/artifcat_entity.py  and updated it. 
Implemented the data ingestion component to read data from MongoDB, save it to a feature store, and split it into train 
and test datasets.Created modular functions for exporting collections as data frames, saving data to CSV files, 
and performing train-test splits. Defined a data ingestion artifact class to encapsulate the output paths of the train and 
test datasets.Demonstrated how to test the data ingestion pipeline end-to-end and verified the creation of expected files 
and logs. 
updated requirements.txt and ran it so it can install the reamining dependancies 
create main.py and incorporated data_ingestion related section.
Now, Data Validation:
Data validation is the process of ensuring that data is accurate, complete, consistent, and formatted correctly before it's used,
stored, or processed. Think of it as a quality gateâ€”only clean, trustworthy data gets through

When reading data from MongoDB, it is crucial that the data schema remains unchanged. For example, if there are ten features 
in the dataset, all ten features must always be present. If any feature is missing or has a different data type, the model 
cannot be trained properly.
Data Drift:
It is also important to check the distribution of features. For instance, if a feature initially follows a normal distribution
but later changes to a different distribution, this is known as data drift. Data drift can significantly impact model performance,
as the new data distribution may differ greatly from the training data.
Steps: 
define dataclass in entity/artifcat_entity.py,
defined constants in constants/traning_pipeline/__init__.py, 
then update config_entity.py/ DataValidationConfig,
then update/create: components/data_validation.py
defined schema file path in constants and created networksecurity/data_schema/schema.yaml to check the schema of the dataset,
that is number of columns and columns data type. we can create this schema file, with the columns and datatypes programmatically
Now, we created utils.py under utils/main_utils  where we will define some common/generic functions like read a yaml file
updated data_validation.py in which we checked schema and data drift. 
Data Transformation:

Configuration entities (config_entity.py) are created to hold paths and parameters for data transformation.
Artifact entities (artifact_entity.py) define the output files generated by the transformation process.
Constants are hardcoded for directory names and parameters such as missing value indicators and KNN Imputer settings.
Utility functions are implemented to save numpy arrays and pickle objects, ensuring directories exist before saving.
The data transformation component imports necessary modules including numpy, pandas, sklearn's KNN Imputer and Pipeline, 
and logging utilities.
The target column is imported from the constants to separate features and labels during transformation.
The data transformation component depends on the data validation artifact to access validated datasets.
removed dill from the requirements.txt as we can save our objects/ numpy array using pickel, implemented 
initiate_data_transformation in data_transformation.py,

Now implementing Model training. The model trainer architecture involves several steps:
update constants/training_pipeline/__init.py__ , defined constants,  then config_entity;
Initialize model trainer configuration  with details like model trainer directory, trained model file path, expected accuracy, 
and model config file path.then updated artifact entity (dataclass), then we started workig on model_trainer.py


updated model

Load the transformed numpy array data from the data transformation artifact.
Split the data into training and testing sets.
Train multiple models on the training data.
Select the best model based on evaluation metrics.
Save the best model as a pickle file.
Calculate metrics to determine the best model.
Combine the trained model and preprocessing pickle files into the model trainer artifact.
This architecture guides the implementation of the model trainer component.

After completing the training, we added mlflow local tracking function in the model training component, then ran "mlflow ui" in 
the terminal. After this we used dagshub to track the models performance with mlflow, we connected with github and connected our
github repository with dagshub, installed dagshub,  gone to remote/code copied it (import code + init code), after this, mlflow will
auomatically logs our experiments on DagsHub. Got url info from dagshub/remote/experiments/mlflow tracking remote, the user id 
is your dagshub account name, and passwrod is available under remote/dvc/secrete Key. Ran <python main.py>)

Model Pusher implementation:
created final_models folder under root folder, wrote the code to save final model, in model_trainer component, saved preprocessing.pickle
object in data_transformation.py into final_model folder, so we will push these artifacts in s3 bucket

created data_training_pipeline.py under pipleline folder, the work we did in main is done using classes and objects.
after that, we moved to app.py where we created fast api. front end will be developed by someone however we will focus on backend. 
added fastapi and uvicorn in requirement.txt. completed app.py, details are written in OneNote, FastApi section. 
after completion, we ran command in terminal:  uvicorn app:app --reload, the browser is opened with some UI, clicked execute on it,
the whole pipleline ran.. and that too without using main.py, so this is another way of doing the thing we did in main.py. 
we can also use this api to collect fresh data, update our mongodb and ran the data indestion again. or run the whole pipleline

Now next step is to create batch prediction, a csv file may be containing data that we need to do prediction, 
we added template folder, and in this, we added table.html,
we added valid_data folder and added test.csv in it. we moved to app.py again, added Jinja template,added 
route "predict", completed it. then ran "uvicorn app:app --reload"  (app is our app name and reload is to 
run the program after changes witout needing to restart it), opened Swagger app and docs via swagger UI
(in the browser).
Now go to the browser using the local address given in the terminal (127.0.0.1:8000), you can see Swagger UI, click Train Route, 
click Try it Out button, and execute.. you can predict by providing the data on which you need prediction. 

Now we will sync our final_model folder to s3 bucket. we can have different models trained on different versions of datasets. Futher,
we have artifacts of each pipeline module, we will also upload the artifacts folder to s3 bucket. 
First we created a file "s3_syncer.py" under networksecurity/cloud where we created two functions. 
Then we created sync_artifact_dir_to_s3 function in training_pipeline.py and then we created sync_saved_model_dir_to_s3 function.
we added these two functions in run_pipeline function in training_pipeline.py. Added self.model_dir in TrainingPipelineConfig
Now we are configuring AWS Cli:
search AWS CLI interface, download and install it. Go to command promtpt, write <aws help> to check it it is working.then we created
an account in AWS, go to IAM Service on AWS, created user "practice_user" and  group "practice_admin", go to users, then security credentials,
create access key, click use case - command line interface, NEXT, provide description, so access key and secret access key for the IAM user 
are generated.
Creat S3 bucket and keep the region same in the whole process
Now in order to set access key go to CLI, type <aws configure>, provide access key (copy from aws site),then provide secret access key,
region: none,format json. press enter. Now run the code to check if the artifacts and models are copied to S3 bucket. 
type <uvicorn app:app --reload>























    