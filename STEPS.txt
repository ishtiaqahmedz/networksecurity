created nenv,gitignore,readme,requirements.txt, then .github/workflows folder under whihc "main.yaml" file (to use github actions
in future), created network_data folder, and notebooks folder, networksecurity underwhich we created our standard folder structure
along with dockerfile and setup.py etc files. After that, we created ".env" file, for using environment variables for whcih we will
use python-dotenv package. 
Now updated requirements.txt file, and use <pip install -r requirements.txt>. Clicked on AutoSave option under FILE menu, so the 
project get saved in case of any changes. We setup  the packages by creating "__init__.py" file under the created folders. We setup
the git repo locally and on github adn pushed the changes.

* updated setup.py, introduced "-e ." in requirements.txt so setup.py may out python project as package. after completing setup.py,
we ran pip install -r requirements.txt, it installed the packages and in last, due to presence of "-e .", it refered to setup.py,
made instllations of our developed packages as well. 
We can add this "-e ." in last when project is matured, this is comunicated by the teacher however ChatGPT and Co-pilot suggest
to add it at begining of the project. 
Now after setup, we work on logging. our folder logging will act as a package as we have __init__.py in it. we created loggin.py 
in that folder. then created exception.py - we completed the code within that and checked if it is working by running 
<python exception.py>

********** ETL PIPELINE - Extraction, Transformation and Load *************

Data may be extracted from different apis, s3 buckets, websites - may be processed adn cleaned and then it can be saved in data desitnations
like MongoDB, MySQL etc. This is usually done by Data Engineering Team 

Our problem Statement: check the website if it is pishing or a legit website. We will read the local available data, do some 
processing and load into MongoDB ATLAS.

I created account, created database on MongoDB, for connecting with MongoDB driver, i selected  python 3.6 or later,
it provided pip command for installation of driver, updated pymongo in requirements.txt, click done. Once cluster is created,
go to CONNECT, click driver, click view full code sample, copy the code and paste into push_data.py in your project, changed
password in the code, saved. python -m pip install "pymongo[srv]==3.6" (as given by MongoDB), changed the working directory 
in the terminal and ran the test_mongodb_conn.py to check if the  connection is pinging. But these details should not be hardcoded in our file
so we will create .env file, so we will create MongoDB URL Key and save our password in it. 

we created push_data.py, loaded .env file, we imported certifi package which is used to issue ssl/tls certificate for network security purpose.
completed and ran push_data.py. then created data_ingestion.py, then config_entity.py and start working on it
under constant, created training_pipeline folder and created __init__.py and start working in this init file - created data_ingestion
block there. 

(Note: Instructor added 0.0.0.0/0 - a generic ip address in MongoDB's IP addressses, he said he did it to make http  connection 
and to cater requests.)
Then we moved back to config_entity.py and worked on it, after that we started working on data_ingestion.py, 

created dataclass DataIngestionArtifact in entity/artifcat_entity.py  and updated it. 
Implemented the data ingestion component to read data from MongoDB, save it to a feature store, and split it into train 
and test datasets.Created modular functions for exporting collections as data frames, saving data to CSV files, 
and performing train-test splits. Defined a data ingestion artifact class to encapsulate the output paths of the train and 
test datasets.Demonstrated how to test the data ingestion pipeline end-to-end and verified the creation of expected files 
and logs. 
updated requirements.txt and ran it so it can install the reamining dependancies 
create main.py and incorporated data_ingestion related section.
Now, Data Validation:
Data validation is the process of ensuring that data is accurate, complete, consistent, and formatted correctly before it's used,
stored, or processed. Think of it as a quality gateâ€”only clean, trustworthy data gets through

When reading data from MongoDB, it is crucial that the data schema remains unchanged. For example, if there are ten features 
in the dataset, all ten features must always be present. If any feature is missing or has a different data type, the model 
cannot be trained properly.
Data Drift:
It is also important to check the distribution of features. For instance, if a feature initially follows a normal distribution
but later changes to a different distribution, this is known as data drift. Data drift can significantly impact model performance,
as the new data distribution may differ greatly from the training data.
Steps: 
define dataclass in entity/artifcat_entity.py,
defined constants in constants/traning_pipeline/__init__.py, 
then update config_entity.py/ DataValidationConfig,
then update/create: components/data_validation.py
defined schema file path in constants and created networksecurity/data_schema/schema.yaml to check the schema of the dataset,
that is number of columns and columns data type. we can create this schema file, with the columns and datatypes programmatically
Now, we created utils.py under utils/main_utils  where we will define some common/generic functions like read a yaml file
updated data_validation.py in which we checked schema and data drift. after data validation, we will now work on data transformation.

















    